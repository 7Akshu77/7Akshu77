{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjalDvdldmus",
        "outputId": "3e24f5b5-a612-4ebc-918e-e065016a0de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.6)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "Successfully installed timm-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6byQ5-Zzefv"
      },
      "outputs": [],
      "source": [
        "from timm.models.layers import to_2tuple, trunc_normal_, DropPath\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozPqcnFAzefv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chan=3, embed_dim=768, multi_conv=False, use_simple_conv=False):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        if multi_conv and not use_simple_conv:\n",
        "            if patch_size[0] == 24:\n",
        "                self.proj = nn.Sequential(\n",
        "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=3, padding=2),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "            elif patch_size[0] == 12:\n",
        "                self.proj = nn.Sequential(\n",
        "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=4, padding=3),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "            elif patch_size[0] == 4:\n",
        "                self.proj = nn.Sequential(\n",
        "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=2, padding=3),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported patch size {patch_size[0]}\")\n",
        "        elif use_simple_conv:\n",
        "            # Simple convolutional embedding\n",
        "            self.proj = nn.Conv2d(in_chan, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        else:\n",
        "            # Default behavior if multi_conv is not used\n",
        "            self.proj = nn.Conv2d(in_chan, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x, extra_padding=False):\n",
        "        B, C, H, W = x.shape\n",
        "        print(f\"Input shape: {x.shape}\")\n",
        "        if extra_padding and (H % self.patch_size[0] != 0 or W % self.patch_size[1] != 0):\n",
        "            p_l = (self.patch_size[1] - W % self.patch_size[1]) // 2\n",
        "            p_r = (self.patch_size[1] - W % self.patch_size[1]) - p_l\n",
        "            p_t = (self.patch_size[0] - H % self.patch_size[0]) // 2\n",
        "            p_b = (self.patch_size[0] - H % self.patch_size[0]) - p_t\n",
        "            x = F.pad(x, (p_l, p_r, p_t, p_b))\n",
        "            print(f\"Padded shape: {x.shape}\")\n",
        "\n",
        "        x = self.proj(x)  # Apply the projection (whether multi-conv or simple-conv)\n",
        "        x = x.flatten(2).transpose(1, 2)  # Flatten and transpose for transformer input\n",
        "\n",
        "        return x\n",
        "\n",
        "# Utility function to convert values to tuple if they aren't already\n",
        "def to_2tuple(x):\n",
        "    if isinstance(x, tuple):\n",
        "        return x\n",
        "    return (x, x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNYvzAZ7zefw"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., patch_size=16):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.wq = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.wk = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.wv = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # Compute Q, K, V matrices\n",
        "        q = self.wq(x[:, 0:1, ...]).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "        k = self.wk(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "        v = self.wv(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # Compute attention output\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, 1, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdfRd3Crzefw"
      },
      "outputs": [],
      "source": [
        "class CrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, has_mlp=True, patch_size=16):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = CrossAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                   attn_drop=attn_drop, proj_drop=drop, patch_size=patch_size)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.has_mlp = has_mlp\n",
        "        if self.has_mlp:\n",
        "            self.norm2 = norm_layer(dim)\n",
        "            mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x[:, 0:1, ...] + self.drop_path(self.attn(self.norm1(x)))\n",
        "        if self.has_mlp:\n",
        "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02dQ58VWzefw"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=attn_drop)\n",
        "        self.drop_path = nn.Identity() if drop_path == 0 else nn.Dropout(drop_path)\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            act_layer(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0])\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        return x\n",
        "class MultiScaleBlock(nn.Module):\n",
        "    def __init__(self, dim, patches, depth, num_heads, mlp_ratio, act_layer=nn.GELU, qkv_bias=False,\n",
        "                 qk_scale=None, attn_drop=0., drop=0., drop_path=0 , norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "\n",
        "        # Creating branches based on the embedding dimension\n",
        "        num_branches = len(dim)\n",
        "        self.num_branches = num_branches\n",
        "\n",
        "        # Transformer block for each branch\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for d in range(num_branches):\n",
        "            temp = []\n",
        "            for i in range(depth[d]):\n",
        "                temp.append(\n",
        "                    Block(dim=dim[d], num_heads=num_heads[d], mlp_ratio=mlp_ratio[d],\n",
        "                          qkv_bias=qkv_bias, drop=drop, attn_drop=attn_drop,\n",
        "                          drop_path=drop_path[i], norm_layer=norm_layer)\n",
        "                )\n",
        "            if len(temp) != 0:\n",
        "                self.blocks.append(nn.Sequential(*temp))\n",
        "        if len(self.blocks) == 0:\n",
        "            self.blocks = None\n",
        "\n",
        "        # Ensuring that all branches are of the same size, creating projection layers if so\n",
        "        self.proje = nn.ModuleList()\n",
        "        for d in range(num_branches):\n",
        "            temp = [norm_layer(dim[d]), act_layer(), nn.Linear(dim[d], dim[(d+1) % num_branches])]\n",
        "            self.proje.append(nn.Sequential(*temp))\n",
        "\n",
        "        self.fusion = nn.ModuleList()\n",
        "        for d in range(num_branches):\n",
        "            d_ = (d + 1) % (num_branches)\n",
        "            nh = num_heads[d_]\n",
        "            if depth[-1] == 0:\n",
        "                self.fusion.append(CrossAttentionBlock(dim=dim[d_], num_heads=nh, mlp_ratio=mlp_ratio[d],\n",
        "                                                       qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop,\n",
        "                                                       attn_drop=attn_drop, drop_path=drop_path[-1],\n",
        "                                                       norm_layer=norm_layer, has_mlp=False,\n",
        "                                                       patch_size=patches[d_]))\n",
        "            else:\n",
        "                temp = []\n",
        "                for _ in range(depth[-1]):\n",
        "                    temp.append(CrossAttentionBlock(dim=dim[d_], num_heads=nh, mlp_ratio=mlp_ratio[d],\n",
        "                                                    qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop,\n",
        "                                                    attn_drop=attn_drop, drop_path=drop_path[-1],\n",
        "                                                    norm_layer=norm_layer, has_mlp=False,\n",
        "                                                    patch_size=patches[d_]))\n",
        "                self.fusion.append(nn.Sequential(*temp))\n",
        "\n",
        "        self.revert_projs = nn.ModuleList()\n",
        "        for d in range(num_branches):\n",
        "            temp = [norm_layer(dim[(d+1) % num_branches]), act_layer(),\n",
        "                    nn.Linear(dim[(d+1) % num_branches], dim[d])]\n",
        "            self.revert_projs.append(nn.Sequential(*temp))\n",
        "\n",
        "    def forward(self, x):\n",
        "        outs_b = [blocks(x_) for x_, blocks in zip(x, self.blocks)]\n",
        "        proj_cls_tok = [proj(x[:, 0:1]) for x, proj in zip(outs_b, self.proje)]\n",
        "\n",
        "        outs = []\n",
        "        for i in range(self.num_branches):\n",
        "            temp = torch.cat((proj_cls_tok[i], outs_b[(i + 1) % self.num_branches][:, 1:, ...]), dim=1)\n",
        "            temp = self.fusion[i](temp)\n",
        "            reverted_pro_cls_tok = self.revert_projs[i](temp[:, 0:1, ...])\n",
        "            temp = torch.cat((reverted_pro_cls_tok, outs_b[i][:, 0:1, ...]), dim=1)\n",
        "        return outs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYT8WFj0zefx",
        "outputId": "dd71ac6c-d759-48d9-ab04-39c3db94587f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "PYJCddVi4-cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeQAqPBmzefx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the Block class\n",
        "\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_chans, num_classes, embed_dim, depth, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm, multi_conv=False):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Flatten the depth list of lists\n",
        "        flat_depth = [item for sublist in depth for item in sublist]\n",
        "\n",
        "        # Calculate dpr based on the flattened depth list\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(flat_depth))]\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            MultiScaleBlock(\n",
        "                dim=embed_dim,\n",
        "                patches=patch_size,\n",
        "                depth=block_cfg,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_scale=qk_scale,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr,  # Pass the drop path here\n",
        "                norm_layer=norm_layer\n",
        "            )\n",
        "            for block_cfg in depth\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.ModuleList([norm_layer(d) for d in embed_dim])\n",
        "        self.head = nn.ModuleList([nn.Linear(d, num_classes) if num_classes > 0 else nn.Identity() for d in embed_dim])\n",
        "\n",
        "        for i in range(len(embed_dim)):\n",
        "            trunc_normal_(self.pos_embed[i], std=.02)\n",
        "            trunc_normal_(self.cls_token[i], std=.02)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        xs = []\n",
        "        for i in range(len(self.embed_dim)):\n",
        "            x_ = torch.nn.functional.interpolate(x, size=(self.img_size[i], self.img_size[i]), mode='bicubic') if H != self.img_size[i] else x\n",
        "            tmp = self.patch_embed[i](x_)\n",
        "            cls_tokens = self.cls_token[i].expand(B, -1, -1)\n",
        "            tmp = torch.cat((cls_tokens, tmp), dim=1)\n",
        "            tmp = tmp + self.pos_embed[i]\n",
        "            tmp = self.pos_drop(tmp)\n",
        "            xs.append(tmp)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            xs = blk(xs)\n",
        "\n",
        "        xs = [self.norm[i](x) for i, x in enumerate(xs)]\n",
        "        out = [x[:, 0] for x in xs]\n",
        "\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        xs = self.forward_features(x)\n",
        "        ce_logits = [self.head[i](x) for i, x in enumerate(xs)]\n",
        "        ce_logits = torch.mean(torch.stack(ce_logits, dim=0), dim=0)\n",
        "        return ce_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P89e2bj4zefx",
        "outputId": "52dcf678-ab68-4667-c6a1-ea3f4758fe7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class labels: ['ash', 'beech', 'cattail', 'cedar', 'clover', 'cyprus', 'daisy', 'dandelion', 'dogwood', 'elm', 'fern', 'fig', 'fir', 'juniper', 'maple', 'poison_ivy', 'sweetgum', 'sycamore', 'trout_lily', 'tulip_tree']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "\n",
        "# Define the paths to your datasets\n",
        "data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/PlantCLEF_Subset\" # Directory containing 'train', 'val', and 'test' folders\n",
        "train_dir = \"/content/gdrive/MyDrive/Colab Notebooks/PlantCLEF_Subset/train\"\n",
        "\n",
        "# Extract class labels from the 'train' folder\n",
        "\n",
        "\n",
        "# Define a custom dataset class to handle the test set\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(root, fname) for fname in os.listdir(root)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, -1  # return a dummy label since the test set doesn't have labels\n",
        "\n",
        "# Define transformations for training and validation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=val_test_transform)\n",
        "test_dataset = TestDataset(root=os.path.join(data_dir, 'test'), transform=val_test_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Verify the class labels\n",
        "print(\"Class labels:\", train_dataset.classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9LpnLrazefy"
      },
      "outputs": [],
      "source": [
        "# Function to train one epoch\n",
        "def train_one_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in data_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader.dataset)\n",
        "    epoch_accuracy = correct / total\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "# Function to validate the model\n",
        "def validate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "qUM5NKyGzefy",
        "outputId": "7f840deb-814a-4d46-985f-ad359a66b068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing with patch size: 24\n",
            "Input shape: torch.Size([1, 3, 224, 224])\n",
            "Input shape: torch.Size([1, 3, 224, 224])\n",
            "Padded shape: torch.Size([1, 3, 240, 240])\n",
            "Patch embeddings raw shape: torch.Size([1, 1600, 768])\n",
            "Padded shape: torch.Size([1, 3, 224, 224]) (no further reshaping required)\n",
            "Patch embeddings shape: torch.Size([1, 1600, 768])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'VisionTransformer' object has no attribute 'pos_embed'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-86c4d6494a2a>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m#cross_attention = CrossAttention(dim=embed_dim, num_heads=num_heads, patch_size=patch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m#attention_output = cross_attention(patch_embeddings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     vision_transformer = VisionTransformer(img_size = [224,240],\n\u001b[0m\u001b[1;32m     42\u001b[0m                                            \u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                                            \u001b[0min_chans\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-7a74b4abed76>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_size, patch_size, in_chans, num_classes, embed_dim, depth, num_heads, mlp_ratio, qkv_bias, qk_scale, drop_rate, attn_drop_rate, drop_path_rate, hybrid_backbone, norm_layer, multi_conv)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mtrunc_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mtrunc_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1729\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'VisionTransformer' object has no attribute 'pos_embed'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "img_size = 224\n",
        "patch_sizes = [24, 12]\n",
        "in_chan = 3\n",
        "embed_dim = 768\n",
        "num_heads = 8\n",
        "\n",
        "for patch_size in patch_sizes:\n",
        "    print(f\"\\nTesting with patch size: {patch_size}\")\n",
        "\n",
        "    # Initialize the PatchEmbed layer\n",
        "    patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chan=in_chan, embed_dim=embed_dim, multi_conv=True)\n",
        "\n",
        "    # Generate a random input tensor simulating an image\n",
        "    x = torch.randn(1, in_chan, img_size, img_size)\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "\n",
        "    # Generate patch embeddings and pad if necessary\n",
        "    patch_embeddings = patch_embed(x, extra_padding=True)\n",
        "\n",
        "    # Check the shape of the patch_embeddings tensor\n",
        "    print(f\"Patch embeddings raw shape: {patch_embeddings.shape}\")\n",
        "\n",
        "    if patch_embeddings.ndimension() == 4:  # If it has the shape (B, C, H', W')\n",
        "        B, C, H_prime, W_prime = patch_embeddings.shape\n",
        "        print(f\"Padded shape: {x.shape}\")\n",
        "        print(f\"Patch embeddings shape: {patch_embeddings.shape}\")\n",
        "\n",
        "        # Reshape for CrossAttention input\n",
        "        N = H_prime * W_prime  # Number of patches\n",
        "        patch_embeddings = patch_embeddings.permute(0, 2, 3, 1).reshape(B, N, C)  # (B, H', W', C) -> (B, N, C)\n",
        "    elif patch_embeddings.ndimension() == 3:  # If it has already been flattened to (B, N, C)\n",
        "        B, N, C = patch_embeddings.shape\n",
        "        print(f\"Padded shape: {x.shape} (no further reshaping required)\")\n",
        "        print(f\"Patch embeddings shape: {patch_embeddings.shape}\")\n",
        "\n",
        "    # Initialize and apply the CrossAttention layer\n",
        "    #cross_attention = CrossAttention(dim=embed_dim, num_heads=num_heads, patch_size=patch_size)\n",
        "    #attention_output = cross_attention(patch_embeddings)\n",
        "    vision_transformer = VisionTransformer(img_size = [224,240],\n",
        "                                           patch_size = [4,12,24],\n",
        "                                           in_chans =3,\n",
        "                                           num_classes = 20,\n",
        "                                           embed_dim = [96,192],\n",
        "                                           depth = [[1, 4, 0], [1, 4, 0], [1, 4, 0]],\n",
        "                                           num_heads = [3,3],\n",
        "                                           mlp_ratio=[4, 4, 1],\n",
        "                                           qkv_bias=True,\n",
        "                                           qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm, multi_conv=False)\n",
        "    print(f\"Attention output shape: {vision_transformer.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chan=3, embed_dim=768, multi_conv=False, use_simple_conv=False):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        if multi_conv and not use_simple_conv:\n",
        "            if patch_size[0] == 24:\n",
        "                self.proj = nn.Sequential(\n",
        "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=3, padding=2),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "            elif patch_size[0] == 12:\n",
        "                self.proj = nn.Sequential(\n",
        "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=4, padding=3),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "            elif patch_size[0] == 4:\n",
        "                self.proj = nn.Sequential(\n",
        "                    nn.Conv2d(in_chan, embed_dim // 4, kernel_size=7, stride=2, padding=3),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported patch size {patch_size[0]}\")\n",
        "        elif use_simple_conv:\n",
        "            # Simple convolutional embedding\n",
        "            self.proj = nn.Conv2d(in_chan, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        else:\n",
        "            # Default behavior if multi_conv is not used\n",
        "            self.proj = nn.Conv2d(in_chan, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x, extra_padding=False):\n",
        "        B, C, H, W = x.shape\n",
        "        if extra_padding and (H % self.patch_size[0] != 0 or W % self.patch_size[1] != 0):\n",
        "            p_l = (self.patch_size[1] - W % self.patch_size[1]) // 2\n",
        "            p_r = (self.patch_size[1] - W % self.patch_size[1]) - p_l\n",
        "            p_t = (self.patch_size[0] - H % self.patch_size[0]) // 2\n",
        "            p_b = (self.patch_size[0] - H % self.patch_size[0]) - p_t\n",
        "            x = F.pad(x, (p_l, p_r, p_t, p_b))\n",
        "\n",
        "        x = self.proj(x)  # Apply the projection (whether multi-conv or simple-conv)\n",
        "        x = x.flatten(2).transpose(1, 2)  # Flatten and transpose for transformer input\n",
        "\n",
        "        return x\n",
        "\n",
        "# Utility function to convert values to tuple if they aren't already\n",
        "def to_2tuple(x):\n",
        "    if isinstance(x, tuple):\n",
        "        return x\n",
        "    return (x, x)\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., patch_size=16):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.wq = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.wk = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.wv = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # Compute Q, K, V matrices\n",
        "        q = self.wq(x[:, 0:1, ...]).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "        k = self.wk(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "        v = self.wv(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # Compute attention output\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, 1, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class CrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, has_mlp=True, patch_size=16):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = CrossAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                   attn_drop=attn_drop, proj_drop=drop, patch_size=patch_size)\n",
        "        self.drop_path = nn.Identity() if drop_path == 0 else nn.Dropout(drop_path)\n",
        "        self.has_mlp = has_mlp\n",
        "        if self.has_mlp:\n",
        "            self.norm2 = norm_layer(dim)\n",
        "            mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "            self.mlp = nn.Sequential(\n",
        "                nn.Linear(dim, mlp_hidden_dim),\n",
        "                act_layer(),\n",
        "                nn.Dropout(drop),\n",
        "                nn.Linear(mlp_hidden_dim, dim),\n",
        "                nn.Dropout(drop),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x[:, 0:1, ...] + self.drop_path(self.attn(self.norm1(x)))\n",
        "        if self.has_mlp:\n",
        "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=attn_drop)\n",
        "        self.drop_path = nn.Identity() if drop_path == 0 else nn.Dropout(drop_path)\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            act_layer(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0])\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class MultiScaleBlock(nn.Module):\n",
        "    def __init__(self, dim, patches, depth, num_heads, mlp_ratio, act_layer=nn.GELU, qkv_bias=False,\n",
        "                 qk_scale=None, attn_drop=0., drop=0., drop_path=0 , norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "\n",
        "        num_branches = len(dim)\n",
        "        self.num_branches = num_branches\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for d in range(num_branches):\n",
        "            temp = []\n",
        "            for i in range(depth[d]):\n",
        "                temp.append(\n",
        "                    Block(dim[d], num_heads[d], mlp_ratio[d], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                          drop=drop, attn_drop=attn_drop, drop_path=drop_path, norm_layer=norm_layer)\n",
        "                )\n",
        "            self.blocks.append(nn.ModuleList(temp))\n",
        "\n",
        "        self.proj = nn.ModuleList()\n",
        "        self.patches = patches\n",
        "\n",
        "        for i in range(len(dim)):\n",
        "            self.proj.append(nn.Linear(dim[i], dim[i + 1]) if i < len(dim) - 1 else nn.Identity())\n",
        "\n",
        "        self.proj.append(nn.Linear(dim[1], dim[0]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        cross_input = x\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            for layer in block:\n",
        "                x = layer(x)\n",
        "\n",
        "            if i + 1 != self.num_branches:\n",
        "                cross_input += self.proj[i](x)\n",
        "\n",
        "        return cross_input\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
        "                 num_heads=12, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., norm_layer=nn.LayerNorm, act_layer=nn.GELU):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chan=in_chans, embed_dim=embed_dim)\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            MultiScaleBlock([embed_dim] * 2, patches=[1] * 2, depth=[depth] * 2, num_heads=[num_heads] * 2,\n",
        "                            mlp_ratio=[mlp_ratio] * 2, norm_layer=norm_layer)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        # Classifier head\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.pos_drop(x + self.pos_embed)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.head(x[:, 0])\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "wFNjCU4U6AJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the paths to your datasets\n",
        "data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/PlantCLEF_Subset\"\n",
        "train_dir = \"/content/gdrive/MyDrive/Colab Notebooks/PlantCLEF_Subset/train\"\n",
        "\n",
        "# Define the custom dataset and transformations\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(root, fname) for fname in os.listdir(root)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, -1  # return a dummy label since the test set doesn't have labels\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=val_test_transform)\n",
        "test_dataset = TestDataset(root=os.path.join(data_dir, 'test'), transform=val_test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Training and validation functions\n",
        "def train_one_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in data_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader.dataset)\n",
        "    epoch_accuracy = correct / total\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "def validate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader.dataset)\n",
        "    epoch_accuracy = correct / total\n",
        "    return epoch_loss, epoch_accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k0PwwX-7AYk",
        "outputId": "dba1eb18-9620-4e5f-c740-e1d41da13843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = VisionTransformer(\n",
        "    img_size=224, patch_size=16, in_chans=3, num_classes=len(train_dataset.classes),\n",
        "    embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "num_epochs=3\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Vm5wMUMz970z",
        "outputId": "ca56d207-c88e-4e0e-d352-8d6e4269b4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5319b7d9fe51>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model = VisionTransformer(\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_chans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXH1LrJMzefy",
        "outputId": "e3c8d225-b9e2-4001-a513-749c34d89436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m797.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/bin/python -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sNIzOnlzefy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "\n",
        "class R2LAttentionPlusFFN(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, kernel_size, num_heads, mlp_ratio=1., qkv_bias=False, qk_scale=None,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, drop_path=0., attn_drop=0., drop=0.,\n",
        "                 cls_attn=True):\n",
        "        super().__init__()\n",
        "\n",
        "        if not isinstance(kernel_size, (tuple, list)):\n",
        "            kernel_size = [(kernel_size, kernel_size), (kernel_size, kernel_size), 0]\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        if cls_attn:\n",
        "            self.norm0 = norm_layer(input_channels)\n",
        "        else:\n",
        "            self.norm0 = None\n",
        "\n",
        "        self.norm1 = norm_layer(input_channels)\n",
        "        self.attn = AttentionWithRelPos(\n",
        "            input_channels, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n",
        "            attn_map_dim=(kernel_size[0][0], kernel_size[0][1]), num_cls_tokens=1)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(input_channels)\n",
        "        self.mlp = Mlp(in_features=input_channels, hidden_features=int(output_channels * mlp_ratio), out_features=output_channels, act_layer=act_layer, drop=drop)\n",
        "\n",
        "        self.expand = nn.Sequential(\n",
        "            norm_layer(input_channels),\n",
        "            act_layer(),\n",
        "            nn.Linear(input_channels, output_channels)\n",
        "        ) if input_channels != output_channels else None\n",
        "        self.output_channels = output_channels\n",
        "        self.input_channels = input_channels\n",
        "\n",
        "    def forward(self, xs):\n",
        "        # xs is expected to be a tuple of (out, B, H, W, mask, additional_patch_tokens)\n",
        "        out, B, H, W, mask, additional_patch_tokens = xs\n",
        "\n",
        "        # Process the output from Cross ViT model\n",
        "        cls_tokens = out[:, 0:1, ...]\n",
        "        C = cls_tokens.shape[-1]\n",
        "        cls_tokens = cls_tokens.reshape(B, -1, C)  # (B)x(H/sxW/s)xC\n",
        "\n",
        "        if self.norm0 is not None:\n",
        "            cls_tokens = cls_tokens + self.drop_path(self.attn(self.norm0(cls_tokens)))  # (B)x(H/sxW/s)xC\n",
        "\n",
        "        # Reshape class tokens for concatenation\n",
        "        cls_tokens = cls_tokens.reshape(-1, 1, C)  # (BxH/sxW/s)x1xC\n",
        "\n",
        "        # Concatenate the processed Cross ViT output with additional 4x4 patch tokens\n",
        "        out = torch.cat((cls_tokens, out[:, 1:, ...]), dim=1)\n",
        "\n",
        "        # Add additional 4x4 patch tokens to the output\n",
        "        out = torch.cat((out, additional_patch_tokens), dim=1)\n",
        "\n",
        "        # Apply attention and feed-forward network on concatenated output\n",
        "        tmp = out\n",
        "        tmp = tmp + self.drop_path(self.attn(self.norm1(tmp), patch_attn=True, mask=mask))\n",
        "        identity = self.expand(tmp) if self.expand is not None else tmp\n",
        "        tmp = identity + self.drop_path(self.mlp(self.norm2(tmp)))\n",
        "\n",
        "        return tmp\n",
        "\n",
        "class AttentionWithRelPos(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.,\n",
        "                 attn_map_dim=None, num_cls_tokens=1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        self.num_cls_tokens = num_cls_tokens\n",
        "        if attn_map_dim is not None:\n",
        "            one_dim = attn_map_dim[0]\n",
        "            rel_pos_dim = (2 * one_dim - 1)\n",
        "            self.rel_pos = nn.Parameter(torch.zeros(num_heads, rel_pos_dim ** 2))\n",
        "            tmp = torch.arange(rel_pos_dim ** 2).reshape((rel_pos_dim, rel_pos_dim))\n",
        "            out = []\n",
        "            offset_x = offset_y = one_dim // 2\n",
        "            for y in range(one_dim):\n",
        "                for x in range(one_dim):\n",
        "                    for dy in range(one_dim):\n",
        "                        for dx in range(one_dim):\n",
        "                            out.append(tmp[dy - y + offset_y, dx - x + offset_x])\n",
        "            self.rel_pos_index = torch.tensor(out, dtype=torch.long)\n",
        "            trunc_normal_(self.rel_pos, std=.02)\n",
        "        else:\n",
        "            self.rel_pos = None\n",
        "\n",
        "    def forward(self, x, patch_attn=False, mask=None):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if self.rel_pos is not None and patch_attn:\n",
        "            # Apply relative positional encoding\n",
        "            rel_pos = self.rel_pos[:, self.rel_pos_index.to(attn.device)].reshape(self.num_heads, N - self.num_cls_tokens, N - self.num_cls_tokens)\n",
        "            attn[:, :, self.num_cls_tokens:, self.num_cls_tokens:] = attn[:, :, self.num_cls_tokens:, self.num_cls_tokens:] + rel_pos\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n",
        "            attn = attn.masked_fill(mask == 0, torch.finfo(attn.dtype).min)\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvr3ik4gzefz"
      },
      "outputs": [],
      "source": [
        "class ConvAttBlock(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, kernel_size, num_blocks, num_heads, mlp_ratio=1., qkv_bias=False, qk_scale=None, pool='sc',\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, drop_path_rate=(0.,), attn_drop_rate=0., drop_rate=0.,\n",
        "                 cls_attn=True, peg=False):\n",
        "        super().__init__()\n",
        "        tmp = []\n",
        "        if pool:\n",
        "            tmp.append(Projection(input_channels, output_channels, act_layer=act_layer, mode=pool))\n",
        "\n",
        "        for i in range(num_blocks):\n",
        "            kernel_size_ = kernel_size\n",
        "            tmp.append(R2LAttentionPlusFFN(output_channels, output_channels, kernel_size_, num_heads, mlp_ratio, qkv_bias, qk_scale,\n",
        "                                           act_layer=act_layer, norm_layer=norm_layer, drop_path=drop_path_rate[i], attn_drop=attn_drop_rate, drop=drop_rate,\n",
        "                                           cls_attn=cls_attn))\n",
        "\n",
        "        self.block = nn.ModuleList(tmp)\n",
        "        self.output_channels = output_channels\n",
        "        self.ws = kernel_size\n",
        "        if not isinstance(kernel_size, (tuple, list)):\n",
        "            kernel_size = [(kernel_size, kernel_size), (kernel_size, kernel_size), 0]\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        self.peg = nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1, groups=output_channels, bias=False) if peg else None\n",
        "\n",
        "    def forward(self, xs):\n",
        "        cls_tokens, patch_tokens = xs\n",
        "        cls_tokens, patch_tokens = self.block[0]((cls_tokens, patch_tokens))\n",
        "        out, mask, p_l, p_r, p_t, p_b, B, C, H, W = convert_to_flatten_layout(cls_tokens, patch_tokens, self.ws)\n",
        "        for i in range(1, len(self.block)):\n",
        "            blk = self.block[i]\n",
        "\n",
        "            out = blk((out, B, H, W, mask))\n",
        "            if self.peg is not None and i == 1:\n",
        "                cls_tokens, patch_tokens = convert_to_spatial_layout(out, self.output_channels, B, H, W, self.kernel_size, mask, p_l, p_r, p_t, p_b)\n",
        "                cls_tokens = cls_tokens + self.peg(cls_tokens)\n",
        "                patch_tokens = patch_tokens + self.peg(patch_tokens)\n",
        "                out, mask, p_l, p_r, p_t, p_b, B, C, H, W = convert_to_flatten_layout(cls_tokens, patch_tokens, self.ws)\n",
        "\n",
        "        cls_tokens, patch_tokens = convert_to_spatial_layout(out, self.output_channels, B, H, W, self.kernel_size, mask, p_l, p_r, p_t, p_b)\n",
        "        return cls_tokens, patch_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk1zFSaazefz"
      },
      "outputs": [],
      "source": [
        "class Projection(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, act_layer, mode='sc'):\n",
        "        super().__init__()\n",
        "        tmp = []\n",
        "        #adds stride and padding to the kernel according to the description\n",
        "        if 'c' in mode:\n",
        "            ks = 2 if 's' in mode else 1\n",
        "            if ks == 2:\n",
        "                stride = ks\n",
        "                ks = ks + 1\n",
        "                padding = ks // 2\n",
        "            else:\n",
        "                stride = ks\n",
        "                padding = 0\n",
        "\n",
        "            if input_channels == output_channels and ks == 1:\n",
        "                tmp.append(nn.Identity())\n",
        "            else:\n",
        "                tmp.extend([\n",
        "                    LayerNorm2d(input_channels),\n",
        "                    act_layer(),\n",
        "                ])\n",
        "                tmp.append(nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=ks, stride=stride, padding=padding, groups=input_channels))\n",
        "\n",
        "        self.proj = nn.Sequential(*tmp)\n",
        "        self.proj_cls = self.proj\n",
        "\n",
        "    def forward(self, xs):\n",
        "        cls_tokens, patch_tokens = xs\n",
        "        # x: BxCxHxW\n",
        "        cls_tokens = self.proj_cls(cls_tokens)\n",
        "        patch_tokens = self.proj(patch_tokens)\n",
        "        return cls_tokens, patch_tokens\n",
        "\n",
        "\n",
        "def convert_to_flatten_layout(cls_tokens, patch_tokens, ws):\n",
        "    # padding if needed, and all paddings are happened at bottom and right.\n",
        "    B, C, H, W = patch_tokens.shape\n",
        "    _, _, H_ks, W_ks = cls_tokens.shape\n",
        "    need_mask = False\n",
        "    p_l, p_r, p_t, p_b = 0, 0, 0, 0\n",
        "    if H % (H_ks * ws) != 0 or W % (W_ks * ws) != 0:\n",
        "        p_l, p_r = 0, W_ks * ws - W\n",
        "        p_t, p_b = 0, H_ks * ws - H\n",
        "        patch_tokens = F.pad(patch_tokens, (p_l, p_r, p_t, p_b))\n",
        "        need_mask = True\n",
        "\n",
        "    B, C, H, W = patch_tokens.shape\n",
        "    kernel_size = (H // H_ks, W // W_ks)\n",
        "    tmp = F.unfold(patch_tokens, kernel_size=kernel_size, stride=kernel_size, padding=(0, 0))\n",
        "    patch_tokens = tmp.transpose(1, 2).reshape(-1, C, kernel_size[0] * kernel_size[1]).transpose(-2, -1)\n",
        "    # If the image is not in correct size then we add 0's to make it same and we have to make sure that the kernel does not use those values so this\n",
        "    #makes all the positions where 0's were added to 0 in keernel as well.\n",
        "    if need_mask:\n",
        "        BH_sK_s, ksks, C = patch_tokens.shape\n",
        "        H_s, W_s = H // ws, W // ws\n",
        "        mask = torch.ones(BH_sK_s // B, 1 + ksks, 1 + ksks, device=patch_tokens.device, dtype=torch.float)\n",
        "        right = torch.zeros(1 + ksks, 1 + ksks, device=patch_tokens.device, dtype=torch.float)\n",
        "        tmp = torch.zeros(ws, ws, device=patch_tokens.device, dtype=torch.float)\n",
        "        tmp[0:(ws - p_r), 0:(ws - p_r)] = 1.\n",
        "        tmp = tmp.repeat(ws, ws)\n",
        "        right[1:, 1:] = tmp\n",
        "        right[0, 0] = 1\n",
        "        right[0, 1:] = torch.tensor([1.] * (ws - p_r) + [0.] * p_r).repeat(ws).to(right.device)\n",
        "        right[1:, 0] = torch.tensor([1.] * (ws - p_r) + [0.] * p_r).repeat(ws).to(right.device)\n",
        "        bottom = torch.zeros_like(right)\n",
        "        bottom[0:ws * (ws - p_b) + 1, 0:ws * (ws - p_b) + 1] = 1.\n",
        "        bottom_right = copy.deepcopy(right)\n",
        "        bottom_right[0:ws * (ws - p_b) + 1, 0:ws * (ws - p_b) + 1] = 1.\n",
        "\n",
        "        mask[W_s - 1:(H_s - 1) * W_s:W_s, ...] = right\n",
        "        mask[(H_s - 1) * W_s:, ...] = bottom\n",
        "        mask[-1, ...] = bottom_right\n",
        "        mask = mask.repeat(B, 1, 1)\n",
        "    else:\n",
        "        mask = None\n",
        "\n",
        "    cls_tokens = cls_tokens.flatten(2).transpose(-2, -1)  # (N)x(H/sxK/s)xC\n",
        "    cls_tokens = cls_tokens.reshape(-1, 1, cls_tokens.size(-1))  # (NxH/sxK/s)x1xC\n",
        "\n",
        "    out = torch.cat((cls_tokens, patch_tokens), dim=1)\n",
        "\n",
        "    return out, mask, p_l, p_r, p_t, p_b, B, C, H, W\n",
        "\n",
        "\n",
        "def convert_to_spatial_layout(out, output_channels, B, H, W, kernel_size, mask, p_l, p_r, p_t, p_b):\n",
        "    \"\"\"\n",
        "    Convert the token layer from flatten into 2-D, will be used to downsample the spatial dimension.\n",
        "    \"\"\"\n",
        "    cls_tokens = out[:, 0:1, ...]\n",
        "    patch_tokens = out[:, 1:, ...]\n",
        "    # cls_tokens: (BxH/sxW/s)x(1)xC, patch_tokens: (BxH/sxW/s)x(ksxks)xC\n",
        "    C = output_channels\n",
        "    kernel_size = kernel_size[0]\n",
        "    H_ks = H // kernel_size[0]\n",
        "    W_ks = W // kernel_size[1]\n",
        "    # reorganize data, need to convert back to cls_tokens: BxCxH/sxW/s, patch_tokens: BxCxHxW\n",
        "    cls_tokens = cls_tokens.reshape(B, -1, C).transpose(-2, -1).reshape(B, C, H_ks, W_ks)\n",
        "    patch_tokens = patch_tokens.transpose(1, 2).reshape((B, -1, kernel_size[0] * kernel_size[1] * C)).transpose(1, 2)\n",
        "    patch_tokens = F.fold(patch_tokens, (H, W), kernel_size=kernel_size, stride=kernel_size, padding=(0, 0))\n",
        "\n",
        "    if mask is not None:\n",
        "        if p_b > 0:\n",
        "            patch_tokens = patch_tokens[:, :, :-p_b, :]\n",
        "        if p_r > 0:\n",
        "            patch_tokens = patch_tokens[:, :, :, :-p_r]\n",
        "\n",
        "    return cls_tokens, patch_tokens"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}